\documentclass[11pt,twoside=semi,openright,numbers=noenddot]{scrartcl}
% \input{/home/amakelov/workspace/tools/latex/preamble-act2020.tex}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
% \usepackage{minted}
% \usemintedstyle{friendly}

\usepackage{listings}
\usepackage{xcolor}

% Define colors for syntax highlighting
\definecolor{keyword}{RGB}{255,0,90}
\definecolor{comment}{RGB}{0,150,0}
\definecolor{string}{RGB}{255,140,0}

% Configure the listings package
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{keyword},
    commentstyle=\color{comment},
    stringstyle=\color{string},
    showstringspaces=false,
    frame=lines,
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    captionpos=b,
}

\title{\texttt{mandala}: Persistent \& Queriable Memoization for Easy and Powerful Scientific Data Management}
\author{Aleksandar Makelov\\ \texttt{aleksandar.makelov@gmail.com}}
\begin{document}
\maketitle


\begin{abstract}
  We present \texttt{mandala}\footnote{\url{https://github.com/amakelov/mandala}}, a Python library that considerably simplifies
  scientific data management and incremental computing in Python. While most
  traditional and/or popular data management solutions are based on \emph{logging},
  \texttt{mandala} takes a fundamentally different path, using
  \emph{memoization} of Python function calls as the fundamental unit of saving,
  loading, querying and deleting computational artifacts. 
  
  It does so by implementing a `compositional' form of memoization, which keeps
  track of how memoized functions compose with one another. In this way, all
  computations in a project form one computational graph, which can be queried
  and manipulated in high-level ways through a \emph{computation frame}, which
  is a natural generalization of a dataframe in a way made precise in this
  paper. 

  Various features implemented on top of the core memoization data structures --
  such as natively and transparently handling Python collections, caching of
  intermediate results, and a flexible versioning system with dynamic dependency
  tracking -- turn \texttt{mandala} into a practical and radically simpler tool
  for managing and interacting with computational data.
\end{abstract}

\section{Introduction}
\label{section:}

Numerical experiments and simulations are growing into a central part of many
areas of science and engineering \cite{hey2009fourth}, with recent trends in
fields such as machine learning moving towards ever-increasing complexity of
computational pipelines, and mission-critical applications such as autonomous
driving and medical diagnosis.

These trends impose opposing constraints on the tools used to manage the
resulting computational artifacts: on the one hand, they should be simple and
easy to use, with a minimal learning curve and unobtrusive syntax and semantics;
on the other hand, they should be powerful enough to enable high-level
operations and full data provenance in complex projects.

In this paper, we approach this tension from first principles, and propose to
use \emph{memoization} of function calls as the basic building block of such a
system:
\begin{itemize}
\item First, we propose a simple design for a decorator for Python functions
which makes them automatically memoize their results in a shared storage. In
this way, outputs of one memoized call can be directly used as inputs to other
memoized calls, without data duplication. This results in a semantics where
every computation is executed at most once, and complex computations can be
retraced call by call in order to get to a particular result, or resume
computation from a particular point. This eliminates code to manually save and
load computational artifacts (as, as a consequence, eliminates decisions on how
to name artifacts), while re-using the familiar Python function call syntax and
semantics.
\item However, memoization by itself is not enough; we need a way to query and
manipulate the stored computation graph in high-level declarative ways. To this
end, we propose \emph{computation frames}, which generalize dataframes in a
natural way to handle memoized computation graphs. The `columns' of a
computation frame are the topology of a computational graph, and the `rows' are
individual computations following that topology (perhaps partially).
Computation frames allow one to explore the stored computation graph
interactively and in a way adapted to one's use case. Importantly, thanks to the
availability of full data provenance in the storage, computation frames can
perform high-level actions such as `delete all computations that depend on these
values' with a single line of code.
\end{itemize}


\section{An Essential vs. Accidental Complexity Analysis}
\label{subsection:}

The complexity involved in a software project can be divided into two main
categories: essential and accidental. Essential complexity is intrinsic to the
problem being solved, while accidental complexity is due to the tools and
techniques used to solve the problem. 

If we adopt this point of view on the problem of scientific computing, we find
that the essential complexity is that of defining computational
primitives, and defining how to compose them in order to compute artifacts of 
interest. However, currently we also must typically (1) specify how to 
execute these computations (e.g., how to make the most use of parallelism and other computational resource), and (2) how to persist and query the results of these computations.

In this work we focus on eliminating component (2) of the accidental complexity.
We begin with the observation that \emph{the way in which a user builds up
program state} (of e.g.\ a Python program or Jupyter notebook) \emph{already
encodes most of the information needed to persist computational artifacts},
albeit in a form different from how we typically think about persistence. In
particular, the state of the program keeps computational artifacts of interest
in memory under the names of local variables, and the way we build up this state
encodes the provenance of these artifacts -- the two key ingredients needed for
persistence.

We therefore aim to build a system that introduces a minimal level of
instrumentation in order to capture the way program state is constructed. Since
it is both difficult and undesirable to persist and track every single
computation that happens in a program, a natural place to inject this
instrumentation is at the level of function calls. This is because function
calls are already a natural and familiar way to encapsulate and compose 
computation in Python.

\section{Core Concepts}
\label{section:}

\subsection{Memoization}

Memoization is a technique that stores the results of expensive function calls.
In \texttt{mandala}, memoization has a few unusual properties. First, multiple
memoized functions share the same storage backend; this allows for the results
of one function to be used as inputs to another function without data
duplication. Second, every memoized object has \emph{two} identifiers: a
\textbf{content hash} and a \textbf{history hash}:
\begin{itemize}
\item The content hash is (modulo hash collisions) a unique identifier of the
value of the object
\item the history hash is a unique identifier (again, modulo hash collisions) of
the full composition of memoized calls that produced the object, together with the content hashes of the inputs to this computation (which must necessarily be values not produced by memoized calls).
\end{itemize}
This allows the following desirable features:
\begin{itemize}
\item We can distinguish between two objects that have the same value but were
produced by different computations. For instance, there could be two memoized
calls, both of which with the result $42$, but produced in entirely different
ways. One may be interested in the provenance of only one of these $42$s.
\item We can de-duplicate storage, by storing only one copy of each unique
content hash, and storing references from history hashes to content hashes.
\end{itemize}

A simple example of using \texttt{mandala} is shown in Figure \ref{fig:basic-usage}.

\begin{figure}
\begin{lstlisting}

In [1]: from mandala._next.imports import *
   ...: from mandala._next.common_imports import *

In [2]: storage = Storage()
   ...: 
   ...: @op
   ...: def inc(eggs):
   ...:     print(f'Executing inc with eggs={eggs}')
   ...:     return eggs + 1
   ...: 
   ...: with storage:
   ...:     spam = inc(eggs=42)
   ...: 
Executing inc with eggs=42

In [3]: @op
   ...: def make_breakfast(spam, eggs):
   ...:     print(f'Executing make_breakfast with spam={spam} and eggs={eggs}')
   ...:     return spam + eggs
   ...: 
   ...: with storage:
   ...:     eggs = 42
   ...:     spam = inc(eggs=eggs)
   ...:     more_spam = inc(eggs=spam)
   ...:     breakfast = make_breakfast(spam=spam, eggs=eggs)
   ...:     breakfast_2 = make_breakfast(spam=more_spam, eggs=eggs)
   ...: 
Executing inc with eggs=43
Executing make_breakfast with spam=43 and eggs=42
Executing make_breakfast with spam=44 and eggs=42

In [4]: cf = storage.cf(make_breakfast)
   ...: cf
Out[4]: 
ComputationFrame with 3 variable(s) (5 unique refs), 1 operation(s) (2 unique calls)
Computational graph:
    output_0 = make_breakfast(spam=spam, eggs=eggs)
Use `.info()` to get more information about the CF, or `.info(*variable_name)`
and `.info(*operation_name)` to get information about specific
variable(s)/operation(s)

In [5]: cf = cf.expand()
   ...: cf.eval()
Extracting tuples from the computation graph:
spam = inc(eggs=eggs_0)
output_0 = make_breakfast(spam=spam, eggs=eggs)...
Out[5]: 
   eggs_0        inc  spam  eggs make_breakfast  output_0
0      43  Call(...)    44    42      Call(...)        86
1      42  Call(...)    43    42      Call(...)        85

\end{lstlisting}
\caption{Basic usage of \texttt{mandala} memoization.}
\label{fig:basic-usage}
\end{figure}

\subsection{Retracing}
\label{subsection:}

The compositional nature of memoization makes it possible to build complex
computations out of calls to memoized functions, turning the entire computation
into an end-to-end-memoized interface to its own intermediate results. The main way to interact with such a computation is through \textbf{retracing}, which means stepping through memoized code with the purpose of resuming from a failure, loading intermediate values, or continuing from a particular point with new computations. A small example of retracing is shown in Figure \ref{fig:basic-usage}.

\section{Computation Frames}
\label{section:}

In order to be able to explore and manipulate the full stored computation graph,
patterns like retracing are insufficient, because they require the complete code
producing part of the graph to be available. Instead, we introduce the concept
of a \emph{computation frame}, which is a high-level interface to the stored
computation graph. A computation frame consists of the following data:
\begin{itemize}
\item \textbf{computation graph}: a directed graph $G=(V,F,E)$ where $V$ are variables, $F$ are functions, and labeled edges $E$ connect from variables to functions using these variables as input (with the edge label being the input name), and from functions to variables that are outputs of these functions (with the edge label being the name of the output).
\item \textbf{instance data}: for each variable $v$, a set of history hashes $H_v$ of values corresponding to this variable, and for each function $f$ a set of memoized calls $C_f$ to $f$ corresponding to this node. 
\end{itemize}
This data is subject to the constraint that, if there exists a call $c\in C_f$
where $f$ is connected via an input/output edge to some variable $v$, then the
value of $v$ corresponding to this call belongs to $H_v$. Computation frames are
very closely related to copresheaves over the category $\textbf{Set}$ (see, for
instance,
\url{https://blog.algebraicjulia.org/post/2020/11/sql-as-hypergraph/}), though we do not explore this connection in this paper.

Computation frames enable a number of high-level operations, such as starting
from a single function and expanding the history/future of its inputs/outputs
(as shown in Figure \ref{fig:basic-usage}), and can also be turned into a
dataframe (where some values may be null for computation paths that are not
present in the stored computation graph). A small example of using computation
frames is shown in Figure \ref{fig:basic-usage}.

\section{Extra Features}
\label{section:}

\subsection{Caching}
To speed up retracing and memoization, it is necessary to avoid frequent reads
and writes to the database backend. To this end, \texttt{mandala} implements a
caching system that accumulates results in memory, with the option to explicitly
flush values to disk at once when the user decides, or automatically at the end
of a \texttt{mandala} context. Similarly, to speed up retracing, call metadata can be pre-loaded.

\subsection{Data Structures}
\label{subsection:}

Python's native collections -- lists, dicts, sets -- can be memoized
transparently by \texttt{mandala}, using customized type annotations inheriting
from e.g. \texttt{List[int]}. By applying this type annotation, the collection
is memoized so that its individual elements get their own content and history
hashes, and the collection as a whole gets a content hash based on the content
hashes of its elements. This allows for the memoization of complex data
structures, such as nested dictionaries or lists of dictionaries, in a way that each of their individual elements can be reused. 

This is also made compatible with the way computation frames trace back the history of their values.

\section{Conclusion}
\label{section:}

While \texttt{mandala} is not very mature yet, it has the potential to
considerably simplify the way scientific data is managed and interacted with in
Python. We hope that this paper has given a good overview of the core concepts
of \texttt{mandala}, and that the reader will be interested in exploring the
library further.


% bibliography
\bibliographystyle{plain}
\bibliography{scipy}

\end{document}
